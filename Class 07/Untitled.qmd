---
title: "Class 07: Machine Learning 1"
author: Kimberly Navarro (A17485724)
format: pdf
toc: true
---


## Background
Today we will begin our exploration of some important machine learning methods, namely **clusterring** and **dimensionality reduction**. 

lets make up some input data for clustering where we know what natural `clusters` are:

The function `rnorm()` can be useful here.

```{r}
hist( rnorm(5000))
```
> Q. Generate 30 random numbers centered at +3 and another 30 centered at -3

```{r}
tmp <- c(rnorm(30, mean=3),
rnorm(30, mean=-3) )

x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```



## K-means clustering

The main function in "base R" for K means clustering is called `kmeans()`: 

```{r}
km <- kmeans(x, centers = 2)
km
```

> Q. What component of the results object details the cluster sizes?

```{r}
km$size
```

>Q. What component of the results object details the cluster center?

>Q. What component of the results object details the cluster membership vector (i.e. our main result of which points lie in which cluster)?

```{r}
km$cluster
```

>Q. Plot our clustering results which ppoints colored by clusters and also add the cluster centers as new points colored blue?

```{r}
plot(x, col=c(km$cluster))
points(km$centers, col="blue", pch=15)
```

> Q. Run `kmeans()` again and this time produce 4 clusters and (call your result object `kd`) and make a results figure ike above?

```{r}
kd <- kmeans(x, centers = 4)
plot(x, col = kd$cluster,
     xlab = "Variable 1", ylab = "Variable 2")
points(kd$centers, col = "blue", pch = 15)
```

The metric 
```{r}
km$tot.withinss
kd$tot.withinss
```


> Q. Let's try different number of K(centers) from 1 to 30 and see what the best result is?

```{r}
i <- 1
ans <- NULL 
for(i in 1:30) {
  ans <- c(ans, kmeans(x, centers = i)$tot.withinss)
}

ans
```

```{r}
plot(ans, typ="o")
```

**Key-poit:** K-means will impose a clustering structure on your data even if it is not there - it will always give you the answer you asked for even if that answer is silly!


## Hierarchial Clustering 

The main function for Hierarchical Clustering is called `hclust()`
Unlike `kmeans()` (which does all the work for you) you can't just pass `hclust()` our raw input data. It needs a "distance matrix" lke the new one returned from the `dist()` function.

```{r}
d <- dist(x)
hc <- hclust(d)
hc
plot(hc)
```

To extract our cluster membership vector from a `hclust()` result object we do have to "cut" our tree at a given height to yield separate "groups"/"branches".

```{r}
plot(hc)
abline(h=8, col="red", lty=2)
```

To do this we us the `cutree()` function on our `hclust()` object:

```{r}
cutree(hc, h=8)
```

```{r}
grps <- cutree (hc, h=8)
```


```{r}
table(grps, km$cluster)
```

## PCA of UK food data 

Import the dataset of food consumption in the UK:

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```

>Q. How many rows and columns are in your new data frame named x? What R functions could you use to answer this question?

```{r}
dim(x)
```

One solution to set the row names us to do it by hand...
```{r}
rownames(x) <- x[,1]
```

To remove the first column I can use the minux index trick:
```{r}
x <- x[,-1]
x
```

A better way to do this is to set the row names to the first column with `read.csv()`

```{r}
x <- read.csv(url, row.names = 1)
x
```

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

Note: I personally prefer the x <- read.csv(url, row.names = 1) way because it seems easier.


## Spotting major differences and trends

Is difficult even in this wee 17D dataset...

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

## Pairs plots and heatmaps

```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)
```
```{r}
library(pheatmap)

pheatmap(as.matrix(x))
```

## PCA to the rescue 

The main PCA function in "base R" is called `prcomp()`. This function wants the transpose of our food data as input (i.e. the foods as columns and countries as rows).

```{r}
pca <- prcomp( t(x) )
```

```{r}
summary(pca)
```

```{r}
my_cols <- c("orange", "red","blue", "darkgreen")
```

To make one f main PCA result figures we turn to `pca$x` the scores along our new PCs. This is called "PC plot" or "score plot" or "Ordineation plot"...

```{r}
library(ggplot2)

ggplot(pca$x) +
  aes(PC1, PC2) +
  geom_point(col=my_cols)

```

The second major result figure is called a "loading plot" of "variable contributions plot" or "weight plot"

```{r}
ggplot(pca$rotation)+
  aes(PC1, rownames(pca$rotation))+
  geom_col()
```

